  0%|                                                                                                                   | 0/6172 [00:00<?, ?it/s]/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Traceback (most recent call last):
  File "/home/cyc2202/xyzo/proj_file/xyzo/em/E_step_ent_metamath.py", line 361, in <module>
    trainer.train()
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/trainer.py", line 3485, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cyc2202/xyzo/proj_file/xyzo/em/E_step_ent_metamath.py", line 211, in compute_loss
    rational = model.generate(input_ids=inputs_ids_q_l, attention_mask=mask_q_l, \
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/generation/utils.py", line 2047, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/generation/utils.py", line 3014, in _sample
    next_token_logits = outputs.logits.clone()[:, -1, :].float()
                        ^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/cyc2202/xyzo/proj_file/xyzo/em/E_step_ent_metamath.py", line 361, in <module>
[rank0]:     trainer.train()
[rank0]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/trainer.py", line 2052, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/trainer.py", line 3485, in training_step
[rank0]:     loss = self.compute_loss(model, inputs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/cyc2202/xyzo/proj_file/xyzo/em/E_step_ent_metamath.py", line 211, in compute_loss
[rank0]:     rational = model.generate(input_ids=inputs_ids_q_l, attention_mask=mask_q_l, \
[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/generation/utils.py", line 2047, in generate
[rank0]:     result = self._sample(
[rank0]:              ^^^^^^^^^^^^^
[rank0]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/generation/utils.py", line 3014, in _sample
[rank0]:     next_token_logits = outputs.logits.clone()[:, -1, :].float()
[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^
[rank0]: KeyboardInterrupt
