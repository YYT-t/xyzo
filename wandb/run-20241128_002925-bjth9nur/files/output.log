trained_model_name: gemma-1.1-7b-it_gsm8k_ent0.05_beam1_dosampleFalse_temp1.0_estep__totalepoch1
/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
query_decode: ['Answer the question based on the following example:\nQuestion: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.\nQuestion: Marie has 98 unread messages on her phone. She decides to clear them by reading 20 messages a day. However, she also gets 6 new messages a day. How many days will it take her to read all her unread messages?', 'Answer the question based on the following example:\nQuestion: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.\nQuestion: Mel is three years younger than Katherine.  When Katherine is two dozen years old, how old will Mel be in years?', 'Answer the question based on the following example:\nQuestion: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.\nQuestion: There are 84 people waiting in line to ride a roller coaster at an amusement park.\xa0 The roller coaster has 7 cars, and each car seats 2 people.\xa0 How many times will the ride operator have to run the roller coaster to give everyone in line a turn?', 'Answer the question based on the following example:\nQuestion: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.\nQuestion: Layla is feeding her pet fish. She has two Goldfish which each get one teaspoon of fish food. Her 3 Swordtails each get 2 teaspoons of food. Her 8 Guppies each eat half a teaspoon of food. How much food does she have to give to her fish in total?']
rational_decode: ['Answer the question based on the following example:\nQuestion: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.\nQuestion: Marie has 98 unread messages on her phone. She decides to clear them by reading 20 messages a day. However, she also gets 6 new messages a day. How many days will it take her to read all her unread messages? Answer: There are 98 - 6 = 92 messages left to be read after a day. So it will take her 92 / 20 = 4.6 days to read all the messages.', 'Answer the question based on the following example:\nQuestion: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.\nQuestion: Mel is three years younger than Katherine.  When Katherine is two dozen years old, how old will Mel be in years?\n\nAnswer: Mel is three years younger than Katherine, so if Katherine is x years old, Mel is x - 3 years old. When Katherine is two dozen years old, Mel will be x - 3 years old.', 'Answer the question based on the following example:\nQuestion: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.\nQuestion: There are 84 people waiting in line to ride a roller coaster at an amusement park.\xa0 The roller coaster has 7 cars, and each car seats 2 people.\xa0 How many times will the ride operator have to run the roller coaster to give everyone in line a turn? Answer: There are 84 people waiting in line and each car seats 2 people. So there are 84 / 2 = 42 people who need a ride. The roller coaster has 7 cars, so the ride operator will need to run the roller coaster 42 / 7 = 6 times to give everyone in line a turn.', 'Answer the question based on the following example:\nQuestion: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.\nQuestion: Layla is feeding her pet fish. She has two Goldfish which each get one teaspoon of fish food. Her 3 Swordtails each get 2 teaspoons of food. Her 8 Guppies each eat half a teaspoon of food. How much food does she have to give to her fish in total? Answer: There are 2 + 3*2 + 8*(1/2) = 13 teaspoons of food for Layla’s fish.']
answer_decode: ['The answer is 7.', 'The answer is 21.', 'The answer is 6.', 'The answer is 12.']
Token indices sequence length is longer than the specified maximum sequence length for this model (282 > 256). Running this sequence through the model will result in indexing errors
xz: tensor([     2,   1261,    573,   2872,   3482,    611,    573,   2412,   3287,
        235292,    108,   9413, 235292,  73605, 235349, 235256,   2001,   1503,
           919,   2149,  23246,  11187,   1178,  11270,   9149, 235349, 235256,
          2001,   1503, 235265,   1927,  11270,   9149,   3895,   1378,  11187,
          1401,   1070,   2001,   1503, 235269,   1284,   1134,    791,    476,
          2001,   1503,  11594,    573,   4191,    576,  40742, 235349, 235256,
        235265,  40742, 235349, 235256,   2247,   1503,    603,  40742,  25685,
        235265,   2250,   1767,  11187,    708,    575,  73605, 235349, 235256,
          2001,   1503, 235336,  10358, 235292,   2456,    708, 235248, 235310,
         11187,    575,  40742, 235349, 235256,   2001,   1503, 235269,    712,
         11270,   9149, 235349, 235256,   1503,    603, 235248, 235310, 235287,
        235284,    963, 235284,    589, 235248, 235274, 235276,  11187,   1497,
        235265,  73605, 235349, 235256,   2001,   1503,    603, 235248, 235304,
         11187,  25270,   1178,  11270,   9149, 235349, 235256, 235269,    712,
          1104,    708, 235248, 235274, 235276,    728, 235248, 235304,    589,
        235248, 235324,  11187,    575,  73605, 235349, 235256,   2001,   1503,
        235265,    108,   9413, 235292,  18857,    919, 235248, 235315, 235321,
        188982,  11922,    611,   1070,   5248, 235265,   2475,  40326,    577,
          3110,   1174,    731,   5438, 235248, 235284, 235276,  11922,    476,
          1744, 235265,   4560, 235269,   1284,   1170,   6803, 235248, 235318,
           888,  11922,    476,   1744, 235265,   2250,   1767,   2705,    877,
           665,   1987,   1070,    577,   1682,    832,   1070, 188982,  11922,
        235336,  10358, 235292,   2456,    708, 235248, 235315, 235321,    728,
        235248, 235318,    589, 235248, 235315, 235284,  11922,   2731,    577,
           614,   1682,   1452,    476,   1744, 235265,   1704,    665,    877,
          1987,   1070, 235248, 235315, 235284,   1148, 235248, 235284, 235276,
           589, 235248, 235310, 235265, 235318,   2705,    577,   1682,    832,
           573,  11922, 235265,    108,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1], device='cuda:2')
xz_labels: tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,  10358, 235292,   2456,    708, 235248, 235315, 235321,    728,
        235248, 235318,    589, 235248, 235315, 235284,  11922,   2731,    577,
           614,   1682,   1452,    476,   1744, 235265,   1704,    665,    877,
          1987,   1070, 235248, 235315, 235284,   1148, 235248, 235284, 235276,
           589, 235248, 235310, 235265, 235318,   2705,    577,   1682,    832,
           573,  11922, 235265,    108,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100], device='cuda:2')
decode x:  <bos>Answer the question based on the following example:
Question: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.
Question: Marie has 98 unread messages on her phone. She decides to clear them by reading 20 messages a day. However, she also gets 6 new messages a day. How many days will it take her to read all her unread messages?<eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>
decode xz:  <bos>Answer the question based on the following example:
Question: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.
Question: Marie has 98 unread messages on her phone. She decides to clear them by reading 20 messages a day. However, she also gets 6 new messages a day. How many days will it take her to read all her unread messages? Answer: There are 98 - 6 = 92 messages left to be read after a day. So it will take her 92 / 20 = 4.6 days to read all the messages.
<eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>
decode xzy:  <bos>Answer the question based on the following example:
Question: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.
Question: Marie has 98 unread messages on her phone. She decides to clear them by reading 20 messages a day. However, she also gets 6 new messages a day. How many days will it take her to read all her unread messages? Answer: There are 98 - 6 = 92 messages left to be read after a day. So it will take her 92 / 20 = 4.6 days to read all the messages.
The answer is 7.<eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>
/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
Could not estimate the number of tokens of the input, floating-point operations will not be computed
query_decode: ['Answer the question based on the following example:\nQuestion: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.\nQuestion: A thirsty traveler found an oasis in the desert. He drank 32 ounces of water. His camel drank seven times as much as he did. There are 128 ounces in a gallon. How many gallons of water did they drink altogether?', 'Answer the question based on the following example:\nQuestion: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.\nQuestion: Three old cars displayed in an auction event have different manufacture dates. The first car, made in 1970,  was made 10 years earlier than the second car. The third car was manufactured 20 years later after the second car was manufactured. Calculate the year that the third car was made.', "Answer the question based on the following example:\nQuestion: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.\nQuestion: At Mario's barbershop haircuts are 50% more expensive during the weekends. If Mario paid $18 for his last haircut on Monday, how much he would have paid the day before?", 'Answer the question based on the following example:\nQuestion: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.\nQuestion: Janet has a business selling custom collars for dogs and cats. If it takes 18 inches of nylon to make a dog collar and 10 inches to make a cat collar, how much nylon does she need to make 9 dog collars and 3 cat collars?']
rational_decode: ['Answer the question based on the following example:\nQuestion: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.\nQuestion: A thirsty traveler found an oasis in the desert. He drank 32 ounces of water. His camel drank seven times as much as he did. There are 128 ounces in a gallon. How many gallons of water did they drink altogether? Answer: The traveler drank 32 ounces of water and the camel drank 7 * 32 = 224 ounces of water. They drank 32 + 224 = 256 ounces of water altogether, which is 2 gallons of water.', 'Answer the question based on the following example:\nQuestion: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.\nQuestion: Three old cars displayed in an auction event have different manufacture dates. The first car, made in 1970,  was made 10 years earlier than the second car. The third car was manufactured 20 years later after the second car was manufactured. Calculate the year that the third car was made. Answer: The second car was manufactured 10 years after the first car, so it was made in 1980. The third car was manufactured 20 years after the second car, so it was made in 2000.', "Answer the question based on the following example:\nQuestion: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.\nQuestion: At Mario's barbershop haircuts are 50% more expensive during the weekends. If Mario paid $18 for his last haircut on Monday, how much he would have paid the day before? Answer: The cost of the haircut on Monday is 18/1.5 = $12.", 'Answer the question based on the following example:\nQuestion: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.\nQuestion: Janet has a business selling custom collars for dogs and cats. If it takes 18 inches of nylon to make a dog collar and 10 inches to make a cat collar, how much nylon does she need to make 9 dog collars and 3 cat collars? Answer: The amount of nylon needed to make a dog collar is 18 inches, and the amount of nylon needed to make a cat collar is 10 inches. So, to make 9 dog collars, she needs 9 * 18 = 162 inches of nylon. To make 3 cat collars, she needs 3 * 10 = 30 inches of nylon. In total, she needs 162 + 30 = 192 inches of nylon to make 9 dog collars and 3 cat collars.']
answer_decode: ['The answer is 2.', 'The answer is 2000.', 'The answer is 27.', 'The answer is 192.']
xz: tensor([     2,   1261,    573,   2872,   3482,    611,    573,   2412,   3287,
        235292,    108,   9413, 235292,  73605, 235349, 235256,   2001,   1503,
           919,   2149,  23246,  11187,   1178,  11270,   9149, 235349, 235256,
          2001,   1503, 235265,   1927,  11270,   9149,   3895,   1378,  11187,
          1401,   1070,   2001,   1503, 235269,   1284,   1134,    791,    476,
          2001,   1503,  11594,    573,   4191,    576,  40742, 235349, 235256,
        235265,  40742, 235349, 235256,   2247,   1503,    603,  40742,  25685,
        235265,   2250,   1767,  11187,    708,    575,  73605, 235349, 235256,
          2001,   1503, 235336,  10358, 235292,   2456,    708, 235248, 235310,
         11187,    575,  40742, 235349, 235256,   2001,   1503, 235269,    712,
         11270,   9149, 235349, 235256,   1503,    603, 235248, 235310, 235287,
        235284,    963, 235284,    589, 235248, 235274, 235276,  11187,   1497,
        235265,  73605, 235349, 235256,   2001,   1503,    603, 235248, 235304,
         11187,  25270,   1178,  11270,   9149, 235349, 235256, 235269,    712,
          1104,    708, 235248, 235274, 235276,    728, 235248, 235304,    589,
        235248, 235324,  11187,    575,  73605, 235349, 235256,   2001,   1503,
        235265,    108,   9413, 235292,    586,  96213,  70280,   1942,    671,
         84397,    575,    573,  21459, 235265,   1315,  51524, 235248, 235304,
        235284,  46921,    576,   2003, 235265,   4340,  49291,  51524,   6861,
          3023,    685,   1683,    685,    693,   1498, 235265,   2456,    708,
        235248, 235274, 235284, 235321,  46921,    575,    476,  47515, 235265,
          2250,   1767,  44886,    576,   2003,   1498,    984,   7182,  29911,
        235336,  10358, 235292,    714,  70280,  51524, 235248, 235304, 235284,
         46921,    576,   2003,    578,    573,  49291,  51524, 235248, 235324,
           649, 235248, 235304, 235284,    589, 235248, 235284, 235284, 235310,
         46921,    576,   2003, 235265,   2365,  51524, 235248, 235304, 235284,
           963, 235248, 235284, 235284, 235310,    589, 235248, 235284, 235308,
        235318,  46921,    576,   2003,  29911, 235269,    948,    603, 235248,
        235284,  44886,    576,   2003, 235265,    108,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1], device='cuda:2')
xz_labels: tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,  10358, 235292,    714,  70280,  51524, 235248, 235304, 235284,
         46921,    576,   2003,    578,    573,  49291,  51524, 235248, 235324,
           649, 235248, 235304, 235284,    589, 235248, 235284, 235284, 235310,
         46921,    576,   2003, 235265,   2365,  51524, 235248, 235304, 235284,
           963, 235248, 235284, 235284, 235310,    589, 235248, 235284, 235308,
        235318,  46921,    576,   2003,  29911, 235269,    948,    603, 235248,
        235284,  44886,    576,   2003, 235265,    108,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100], device='cuda:2')
decode x:  <bos>Answer the question based on the following example:
Question: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.
Question: A thirsty traveler found an oasis in the desert. He drank 32 ounces of water. His camel drank seven times as much as he did. There are 128 ounces in a gallon. How many gallons of water did they drink altogether?<eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>
decode xz:  <bos>Answer the question based on the following example:
Question: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.
Question: A thirsty traveler found an oasis in the desert. He drank 32 ounces of water. His camel drank seven times as much as he did. There are 128 ounces in a gallon. How many gallons of water did they drink altogether? Answer: The traveler drank 32 ounces of water and the camel drank 7 * 32 = 224 ounces of water. They drank 32 + 224 = 256 ounces of water altogether, which is 2 gallons of water.
<eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>
decode xzy:  <bos>Answer the question based on the following example:
Question: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name? Answer: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = 10 letters long. Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = 7 letters in Samantha’s last name.
Question: A thirsty traveler found an oasis in the desert. He drank 32 ounces of water. His camel drank seven times as much as he did. There are 128 ounces in a gallon. How many gallons of water did they drink altogether? Answer: The traveler drank 32 ounces of water and the camel drank 7 * 32 = 224 ounces of water. They drank 32 + 224 = 256 ounces of water altogether, which is 2 gallons of water.
The answer is 2.<eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>
Traceback (most recent call last):
  File "/home/cyc2202/xyzo/proj_file/xyzo/E_step_ent.py", line 363, in <module>
    trainer.train()
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/trainer.py", line 3485, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cyc2202/xyzo/proj_file/xyzo/E_step_ent.py", line 210, in compute_loss
    rational = model.generate(input_ids=inputs_ids_q_l, attention_mask=mask_q_l, \
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/generation/utils.py", line 2047, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/generation/utils.py", line 3007, in _sample
    outputs = self(**model_inputs, return_dict=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1603, in _call_impl
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/models/gemma/modeling_gemma.py", line 1069, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1603, in _call_impl
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/models/gemma/modeling_gemma.py", line 847, in forward
    causal_mask = self._update_causal_mask(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/models/gemma/modeling_gemma.py", line 948, in _update_causal_mask
    min_dtype = torch.finfo(dtype).min
                ^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/cyc2202/xyzo/proj_file/xyzo/E_step_ent.py", line 363, in <module>
[rank2]:     trainer.train()
[rank2]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/trainer.py", line 2052, in train
[rank2]:     return inner_training_loop(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/trainer.py", line 3485, in training_step
[rank2]:     loss = self.compute_loss(model, inputs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/cyc2202/xyzo/proj_file/xyzo/E_step_ent.py", line 210, in compute_loss
[rank2]:     rational = model.generate(input_ids=inputs_ids_q_l, attention_mask=mask_q_l, \
[rank2]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/generation/utils.py", line 2047, in generate
[rank2]:     result = self._sample(
[rank2]:              ^^^^^^^^^^^^^
[rank2]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/generation/utils.py", line 3007, in _sample
[rank2]:     outputs = self(**model_inputs, return_dict=True)
[rank2]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1603, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/models/gemma/modeling_gemma.py", line 1069, in forward
[rank2]:     outputs = self.model(
[rank2]:               ^^^^^^^^^^^
[rank2]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1603, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/models/gemma/modeling_gemma.py", line 847, in forward
[rank2]:     causal_mask = self._update_causal_mask(
[rank2]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/cyc2202/anaconda3/envs/yuan/lib/python3.11/site-packages/transformers/models/gemma/modeling_gemma.py", line 948, in _update_causal_mask
[rank2]:     min_dtype = torch.finfo(dtype).min
[rank2]:                 ^^^^^^^^^^^^^^^^^^
[rank2]: KeyboardInterrupt
